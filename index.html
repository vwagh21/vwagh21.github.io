<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Vaidehi Wagh</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- Font Awesome for icons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" integrity="sha512-aXUp+ZL77h9+WPt0H7clu3ghD+O+S1Vqhjf3gjF3+7M8rN8k6zF+O4qZt3/JGc0JffRtgrt0+2K1w4GxRnl+eA==" crossorigin="anonymous" referrerpolicy="no-referrer" />

  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 2rem;
      max-width: 1000px;
      margin: auto;
      line-height: 1.6;
    }

    .container {
      display: flex;
      gap: 3rem;
      flex-wrap: wrap;
    }

    /* LEFT COLUMN */
    .left {
      width: 220px;
      flex-shrink: 0;
    }

    .carousel {
      width: 100%;
      margin-bottom: 1rem;
    }

    .carousel img {
      width: 100%;
      border-radius: 6px;
      display: none;
    }

    .carousel img.active {
      display: block;
    }

    .profile-info {
      list-style: none;
      padding: 0;
      margin: 0;
    }

    .profile-info .name {
      font-size: 1.5rem;
      font-weight: bold;
      margin-bottom: 0.2rem;
    }

    .profile-info .subtitle {
      font-size: 1.2rem;
      color: #555;
      margin-bottom: 0.5rem;
    }

    .profile-info .role {
    font-size: 1rem;   /* slightly smaller than subtitle */
    color: #666;       /* slightly lighter gray */
    margin-bottom: 0.3rem;
    font-style: italic; /* optional, gives a subtle professional look */
  }

    .profile-info li a {
      font-size: 0.95rem;
      text-decoration: none;
      color: #0066cc;
      margin-bottom: 0.3rem;
      display: inline-flex;  /* keeps text inline */
      align-items: center;
    }

    .profile-info li a i {
      margin-right: 0.4rem;
    }

    /* RIGHT COLUMN */
    .right {
      flex: 1;
      min-width: 300px;
    }

    h1, h2 {
      margin-top: 0;
      margin-bottom: 0.5rem;
    }

    /* Cards shared style */
    .cards {
      display: flex;
      flex-direction: column;
      gap: 1.5rem;
    }

    .card {
      display: flex;
      gap: 1rem;
      align-items: flex-start;
    }

    .card img {
      width: 120px;
      height: 90px;
      object-fit: cover;
      border-radius: 6px;
      flex-shrink: 0;
    }

    .card-info p {
      margin: 0.2rem 0 0 0;
      font-size: 0.95rem;
      color: #333;
    }

    /* Publications */
    .publications p {
      margin: 0.5rem 0;
      font-size: 0.95rem;
      line-height: 1.4;
    }

    /* Responsive */
    @media (max-width: 600px) {
      .container {
        flex-direction: column;
      }
      .card {
        flex-direction: column;
        align-items: flex-start;
      }
      .card img {
        width: 100%;
        height: auto;
      }
    }
  </style>
</head>

<body>
  <div class="container">

    <!-- LEFT COLUMN -->
    <div class="left">
      <div class="carousel">
        <img src="images/profpic1.jpg" class="active" alt="Profile photo 1">
        <img src="images/profpic2.jpg" alt="Profile photo 2">
        <img src="images/profpic3.jpg" alt="Profile photo 3">
      </div>

      <ul class="profile-info">
  <li class="name">
    Vaidehi Wagh 
    <span title="VY-day-hee WAHG" style="cursor: help; font-size:0.8em;">&#9432;</span>
  </li>
  <li class="subtitle">MS in Robotics, CMU</li>
  <li class="subtitle">ML · Robotics · HRI</li>
  <li><hr></li>
  <li class="role">GRA @ <a href="https://metamobility.cmu.edu/home">MetaMobility Lab</a></li>
  <li class="role role-header">TA</li>
  <li class="role role-sub">
    <a href="https://cmu-mmml.github.io/spring2024/">Multimodal ML</a> · S26
  </li>
  <li class="role role-sub">
    <a href="https://sites.google.com/andrew.cmu.edu/16385fall2025/home?authuser=0">Computer Vision</a> · F25
  </li>
  <li class="role">Tutor @ <a href="https://www.tutors.plus/">PLUS</a>, S25-F26</li>
  <li><hr></li>
  <li><a href="Wagh_Vaidehi_web_version.pdf"><i class="fas fa-file"></i> CV</a></li>
  <li><a href="https://www.linkedin.com/in/vaidehi-wagh-b08286197"><i class="fab fa-linkedin"></i> LinkedIn</a></li>
  <li><a href="mailto:vwagh@cs.cmu.edu"><i class="fas fa-envelope"></i> Email</a></li>
  <li><a href="https://github.com/vwagh21"><i class="fab fa-github"></i> GitHub</a></li>
  <li><a href="https://scholar.google.com/citations?user=A9XFkDcAAAAJ"><i class="fas fa-graduation-cap"></i> Google Scholar</a></li>
</ul>



    </div>

    <!-- RIGHT COLUMN -->
    <div class="right">
      <h2>About</h2>
      <p>
        I’m a second-year Master’s student in Robotics at Carnegie Mellon University, 
        working with <a href="https://metamobility.cmu.edu/people/inseung-kang">Dr. Inseung Kang</a> 
        in the <a href="https://metamobility.cmu.edu/home">MetaMobility Lab</a>. My research focuses on personalizing wearable robot control 
        using large and small language models deployed on the edge, acting as natural language human–robot interfaces between the user and exoskeleton. 
        My prior work was in temporal deep learning and transfer learning for exo control personalization, where I used minimal subject specific data to 
        adapt pre-existing exo control models to clinical populations.

      </p> 
      <p> 
        Before CMU, I worked with <a href="https://news.ok.ubc.ca/2024/01/22/ubco-research-gives-stroke-survivors-control/">Dr. Sarah Kraeutner</a> 
        at the University of British Columbia, validating computer-vision-based kinematic assessment tools for post-stroke rehabilitation, and spent time 
        as a Financial Analyst at Deloitte. I completed my Bachelor’s in Mechanical Engineering at COEP Tech University in Pune, India.
      </p>
      <p>
        I have experience spanning computer vision, mechatronics, and edge deployment, 
        and I’m broadly interested in ML, AI and robotics technologies that meaningfully improve lives. 
      </p>
      <p>
        <strong>I am seeking full-time opportunities in ML, AI, robotics, or research engineering starting May 2026.</strong>
      </p>

      <!-- Research Section -->
      <h2>Research</h2>
      <div class="cards research">
        <div class="card">
          <img src="images/research1.gif" alt="Personalized Exoskeleton Control">
          <div class="card-info">
            <strong>LLM-enabled user feedback based exoskeleton personalization</strong>
            <p><em>SLMs, Phi2, Whisper-tiny, Jetson Orin Nano</em></p>
            <p>Deployed small language models (Phi2, Sheared-LlaMa 3.2) for on-edge (Jetson Orin), closed-loop, lower limb exoskeleton control, 
            enabling real-time personalization based on user feedback and seamless human–robot interaction.</p>
          </div>
        </div>

        <div class="card">
          <img src="images/research1.gif" alt="Human-Robot Interaction Interfaces">
          <div class="card-info">
            <strong> User personalization of exoskeleton control for stroke populations</strong>
            <p><em>Transfer learning, Temporal ML, Deep learning, TCNs</em></p>
            <p>Applied transfer learning to personalize models for post-stroke individuals, 
              achieving a 27% gain in prediction accuracy with < 0.2% subject-specific data.</p>
          </div>
        </div>

        <div class="card">
          <img src="images/nimbl2.gif" alt="Edge Deployment of AI Models">
          <div class="card-info">
            <strong>Clinical use validation of computer vision software for post-stroke assessment</strong>
            <p><em>MediaPipe, R, Matlab, Kinematics</em></p>
            <p>Deployed a motion tracking pipeline using OpenCV and MediaPipe pose estimation to analyse upper-limb kinematics (~0.14M actions, ~0.3M frames) & identify key markers of maladaptive compensation strategies in post-stroke individuals.</p>
          </div>
        </div>

        <div class="card">
          <img src="images/nimbl2.gif" alt="NIMBL research exp 1">
          <div class="card-info">
            <strong>Clinical use validation of computer vision software for fine limb motion capture</strong>
            <p><em>MediaPipe, R, Matlab, Python</em></p>
            <p>Led a validation study comparing low-cost hand tracking to traditional motion capture using shape similarity analysis (16K fine upper-limb motion trajectories, ~70% accuracy) demonstrating a scalable solution for monitoring fine motor movements in clinical settings.</p>
          </div>
        </div>
        
      </div>

      <!-- Projects Section -->
      <h2>Projects</h2>
      <div class="cards projects">

        <div class="card">
          <img src="images/ttr.png" alt="LLM Human-Robot Interface">
          <div class="card-info">
            <strong> VLM-based Human-Robot Voice Interface – Talking to Robots</strong>
            <p><em>SLMs, Phi4, SFT, Jetson Orin Nano</em></p>
            <p>Developed a human-robot interface using speech and vision inputs to finetune a mini VLM for long-horizon planning and execution of user-personalized lower limb exoskeleton control.</p>
          </div>
        </div>

        <div class="card">
          <img src="images/iv-bot.jpeg" alt="Automated IV Insertion Bot">
          <div class="card-info">
            <strong>Automated IV Insertion Bot – Medical Robotics</strong>
            <p><em>Autodesk, CAD, 3D Printing, Electronics, Control</em></p>
            <p>Designed linear-actuation mechanisms for needle insertion, integrating Arduino control, computer vision-based vein localization, and arm control.</p>
          </div>
        </div>

        <div class="card">
          <img src="images/gail.gif" alt="Generative Adversarial Imitation Learning">
          <div class="card-info">
            <strong>Generative Adversarial Imitation Learning – Intro to Robot Learning</strong>
            <p><em>Inverse reinforcement learning, GAIL</em></p>
            <p>Applied inverse reinforcement learning with GAN-based reward models to simulate human locomotion, exploring state-level and trajectory-level GAIL approaches.</p>
          </div>
        </div>

        <div class="card">
          <img src="images/ocrl.gif" alt="iLQR for learning balance">
          <div class="card-info">
            <strong> LQR for humanoid balancing – Optimal Control and Reinforcement Learning</strong>
            <p><em> MuJoCo, LQR</em></p>
            <p>Designed an LQR-based controller to stabilize a human model balancing on a dynamically moving platform in MuJoCo.</p>
          </div>
        </div>
      </div>

      <!-- Publications -->
      <h2>Publications & Presentations</h2>
      <div class="publications">
        <p>
          [1] <em><strong>Wagh, V.</strong>, Park, D., Young, A. J., & Kang, I.</em> Transfer learning for biological joint moment estimation in stroke populations. <a href="https://asbweb.org/wp-content/uploads/ASB-2025-Abstract-Book-Posters-v2.pdf">Abstract</a>
        </p>
      
        <p>
          [2] <em><strong>Wagh, V.</strong>, Scott, M. W., Andrushko, J. W., Jones, C. B., Boyd, L.A, & Kraeutner, S. N.</em> Using MediaPipe to track upper-limb movement after stroke: a proof-of-principle study. <a href="https://link.springer.com/article/10.1186/s12984-025-01808-4">Journal Paper</a>
        </p>
      
        <p>
          [3] <em><strong>Wagh, V.</strong>, Scott, M. W., & Kraeutner, S. N.</em> Quantifying Similarities Between MediaPipe and a Known Standard to Address Issues in Tracking 2D Upper Limb Trajectories: Proof of Concept Study. <a href="https://formative.jmir.org/2024/1/e56682/">Journal Paper</a>
        </p>
      </div>

    </div>
  </div>

  <!-- CAROUSEL SCRIPT -->
  <script>
    const images = document.querySelectorAll('.carousel img');
    let current = 0;

    setInterval(() => {
      images[current].classList.remove('active');
      current = (current + 1) % images.length;
      images[current].classList.add('active');
    }, 3000);
  </script>
</body>
</html>
